{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "\n",
    "class SatelliteToMapDataset(Dataset):\n",
    "    def __init__(self, root_dir, n_colors=10, resize=None, augmentation=True, plot=False, output_type='map', n_samples_per_image=5000, load=True, erosion_size=3, dilation_size=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.n_colors = n_colors\n",
    "        self.erosion_size = erosion_size\n",
    "        self.dilation_size = dilation_size\n",
    "        self.load = load\n",
    "        self.n_samples_per_image = n_samples_per_image\n",
    "        self.output_type = output_type\n",
    "        self.color_centers = self.get_color_centers()\n",
    "        self.file_list = glob.glob(self.root_dir + '/*.jpg')\n",
    "        self.transform = self.get_transforms(resize, augmentation)\n",
    "        self.transform_mask = self.get_transforms_mask(resize, augmentation)\n",
    "        self.plot = plot\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_list[idx])\n",
    "        w, h = img.size\n",
    "        \n",
    "        img_A, img_B, img_masks = self.transform_set(img.crop((0, 0, w / 2, h)), img.crop((w / 2, 0, w, h)), random.randint(0, 2**32))\n",
    "        \n",
    "        if self.plot:\n",
    "            self.plot_image_pair(img_A, img_B, img_masks)\n",
    "\n",
    "        if self.output_type == 'map':\n",
    "            return {'satellite_image': img_A, 'map_image': img_B}\n",
    "        \n",
    "        return {'satellite_image': img_A, 'masks_image': img_masks}\n",
    "    \n",
    "    def transform_set(self, img_A, img_B, seed):\n",
    "        \n",
    "        masks = self.create_masks_from_image(img_B, self.color_centers) if self.output_type == 'mask' else []\n",
    "        transformed_masks = []\n",
    "        \n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        \n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        img_B = self.transform(img_B)\n",
    "        \n",
    "        for mask in masks:\n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "            transformed_masks.append(self.transform_mask(mask))\n",
    "            \n",
    "        if self.output_type == 'mask':\n",
    "            transformed_masks = np.stack([self.clean_mask(mask) for mask in transformed_masks], axis=0)\n",
    "            transformed_masks = torch.from_numpy(transformed_masks).float()\n",
    "\n",
    "        return img_A, img_B, transformed_masks\n",
    "    \n",
    "    def clean_mask(self, mask):\n",
    "        kernel_erode = np.ones((self.erosion_size, self.erosion_size), np.uint8)\n",
    "        kernel_dilate = np.ones((self.dilation_size, self.dilation_size), np.uint8)\n",
    "\n",
    "        # Convert torch tensor to numpy if necessary\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.numpy()\n",
    "            \n",
    "        if mask.ndim == 3 and mask.shape[0] == 1:  # for single-channel 3D tensor\n",
    "            mask = mask.squeeze(0)  # remove channel dimension\n",
    "            \n",
    "        mask = mask.astype(np.uint8)\n",
    "\n",
    "        if self.erosion_size > 0:\n",
    "            mask = cv2.erode(mask, kernel_erode, iterations=1)\n",
    "        if self.dilation_size > 0:\n",
    "            mask = cv2.dilate(mask, kernel_dilate, iterations=1)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def get_transforms(self, resize=None, augmentation=True):\n",
    "        transform_list = []\n",
    "\n",
    "        if resize is not None:\n",
    "            transform_list.append(transforms.Resize(resize))\n",
    "\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "\n",
    "        if augmentation:\n",
    "            transform_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10),\n",
    "                transforms.RandomPerspective(distortion_scale=0.1, p=0.4),\n",
    "                transforms.RandomErasing(p=0.4, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "            ])\n",
    "\n",
    "        transform_list.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "    \n",
    "    def get_transforms_mask(self, resize=None, augmentation=True):\n",
    "        transform_list = []\n",
    "\n",
    "        if resize is not None:\n",
    "            transform_list.append(transforms.Resize(resize))\n",
    "\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "\n",
    "        if augmentation:\n",
    "            transform_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10),\n",
    "                transforms.RandomPerspective(distortion_scale=0.1, p=0.4),\n",
    "                transforms.RandomErasing(p=0.4, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "            ])\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "    \n",
    "    def plot_image_pair(self, satellite_image, map_image, masks):\n",
    "        # Convert the images back to PIL for display if they are tensors\n",
    "        if isinstance(satellite_image, torch.Tensor):\n",
    "            satellite_image = transforms.ToPILImage()(satellite_image)\n",
    "        if isinstance(map_image, torch.Tensor):\n",
    "            map_image = transforms.ToPILImage()(map_image)\n",
    "\n",
    "        # Determine the number of rows required for subplots based on the number of masks\n",
    "        num_masks = len(masks)\n",
    "        num_rows = (num_masks + 2 + 1) // 2  # 2 images (satellite, map) and masks\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(num_rows, 2, figsize=(12, 6 * num_rows))\n",
    "        \n",
    "        if num_rows > 1:\n",
    "            axs[0, 0].imshow(satellite_image)\n",
    "            axs[0, 0].set_title(\"Satellite Image\")\n",
    "            axs[0, 0].axis('off')\n",
    "\n",
    "            axs[0, 1].imshow(map_image)\n",
    "            axs[0, 1].set_title(\"Map Image\")\n",
    "            axs[0, 1].axis('off')\n",
    "        else:\n",
    "            axs[0].imshow(satellite_image)\n",
    "            axs[0].set_title(\"Satellite Image\")\n",
    "            axs[0].axis('off')\n",
    "\n",
    "            axs[1].imshow(map_image)\n",
    "            axs[1].set_title(\"Map Image\")\n",
    "            axs[1].axis('off')\n",
    "\n",
    "        # Plot each mask\n",
    "        for i, mask in enumerate(masks):\n",
    "            row = (i + 2) // 2\n",
    "            col = (i + 2) % 2\n",
    "            \n",
    "            if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "                mask = mask.squeeze(0)  # Remove the channel dimension\n",
    "\n",
    "            axs[row, col].imshow(mask, cmap='gray')\n",
    "            axs[row, col].set_title(f\"Mask {i + 1}\")\n",
    "            axs[row, col].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def find_representative_colors(self):\n",
    "        # Initialize an empty array for storing sampled pixels\n",
    "        sample_pixels = np.empty((0, 3), dtype=int)\n",
    "\n",
    "        for img_name in os.listdir(self.root_dir):\n",
    "            img = Image.open(os.path.join(self.root_dir, img_name))\n",
    "            \n",
    "            w, h = img.size\n",
    "            img = img.crop((w // 2, 0, w, h))  # Crop the right half of the image\n",
    "\n",
    "            # Convert to NumPy array\n",
    "            np_img = np.array(img)\n",
    "\n",
    "            # Randomly sample pixels from the image\n",
    "            idx = np.random.choice(np_img.shape[0] * np_img.shape[1], self.n_samples_per_image, replace=False)\n",
    "            sampled_pixels = np_img.reshape(-1, 3)[idx]\n",
    "\n",
    "            # Append sampled pixels to the array\n",
    "            sample_pixels = np.vstack((sample_pixels, sampled_pixels))\n",
    "\n",
    "        # Apply KMeans to find color centers\n",
    "        # Explicitly setting n_init to 'auto' to suppress the warning and future-proof the code\n",
    "        kmeans = KMeans(n_clusters=self.n_colors, random_state=0, n_init='auto').fit(sample_pixels)\n",
    "        return kmeans.cluster_centers_\n",
    "    \n",
    "    def create_masks_from_image(self, image, color_centers):\n",
    "        # Convert PIL Image to NumPy array\n",
    "        np_image = np.array(image)\n",
    "        # Reshape image and assign each pixel to the nearest color center\n",
    "        pixels = np_image.reshape(-1, 3)\n",
    "        \n",
    "        nearest_centers = np.argmin(np.linalg.norm(pixels - color_centers[:, np.newaxis], axis=2), axis=0)\n",
    "        clustered_image = color_centers[nearest_centers].reshape(np_image.shape).astype(np.uint8)\n",
    "\n",
    "        # Split masks into binary masks for each color center\n",
    "        masks = nearest_centers.reshape(np_image.shape[:2])\n",
    "        binary_masks = [(masks == i).astype(int) for i in range(len(color_centers))]\n",
    "        \n",
    "        return binary_masks\n",
    "    \n",
    "    def visualize_color_centers(self, color_centers):\n",
    "        # Create a figure and a set of subplots\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Add a rectangle patch for each color\n",
    "        for i, color in enumerate(color_centers):\n",
    "            # Normalize color values to [0, 1] as expected by matplotlib\n",
    "            color_normalized = color / 255.0\n",
    "            rect = patches.Rectangle((i, 0), 1, 1, linewidth=1, edgecolor='r', facecolor=color_normalized)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # Set the limits and aspect of the plot\n",
    "        ax.set_xlim(0, self.n_colors)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        plt.axis('off')  # Turn off the axis\n",
    "\n",
    "        plt.savefig(\"models/color_centers.png\")\n",
    "        \n",
    "    def get_color_centers(self):\n",
    "        try:\n",
    "            assert self.load == True\n",
    "            with open(\"models/segmentation_thresholds.json\", \"r\") as f:\n",
    "                color_centers = json.load(f)\n",
    "            assert len(color_centers) == self.n_colors\n",
    "        except:\n",
    "            color_centers = self.find_representative_colors()\n",
    "            self.visualize_color_centers(color_centers)\n",
    "            \n",
    "            with open(\"models/segmentation_thresholds.json\", \"w\") as f:\n",
    "                json.dump(color_centers.tolist(), f)\n",
    "            \n",
    "        return np.array(color_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SatelliteToMapDataset(\"data/train_0\", erosion_size=1, dilation_size=1, plot=True, output_type='mask', n_samples_per_image=100000, n_colors=9, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[222.54114006, 218.19486678, 209.03655732],\n",
    "       [176.39517098, 207.36533546, 253.68176394],\n",
    "       [242.3009299 , 240.07981674, 234.13577431],\n",
    "       [245.81090326, 157.79299076,  38.5570436 ],\n",
    "       [202.48862628, 222.39955981, 172.18012607],\n",
    "       [232.15995471, 228.71470255, 222.00992067],\n",
    "       [253.46324374, 253.25226467, 251.16236616],\n",
    "       [247.36335589, 220.42228868, 136.22587754],\n",
    "       [200.00481359, 205.19318739, 189.33936565],\n",
    "       [230.09122322, 177.85173569,  78.10472596]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
